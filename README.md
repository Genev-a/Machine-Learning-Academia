# Master of Statistics Portfolio üìä

Welcome to my Master of Statistics project repository! Here, you'll find a collection of some homeworks, exams and presentations completed during my Master of Statistic's program at California State University, Fullerton. Each project showcases applications of statistical methods and machine learning techniques to solve real-world problems.

I graduated with the higher distinction: summa cum laude and a 4.0 GPA, ranking #1 in my cohort. During the course of the program, I received 3 awards for my academic excellence & extracuricular activities:
- **Exceptional Graduate Student Award**: Award granted to honor graduating students who have demonstrated high academic achievement and/or outstanding accomplishments and service in the department.
- **Distinguished Phi Kappa Phi Member Award**: Monetary award offered to 10 Phi Kappa Phi student members who have excelled in their studies and extracurricular activities.
- **Non Residential Fee Waiver**: Highly competitive scholarship, based on academic excellence. Granted to very few students by the Department of Mathematics for the whole duration of their studies.

## Table of Contents üìë

- [About the Repository](#about-the-repository-)
- [Academic Work](#academic-work-)
- [Technologies Used](#technologies-used-)
- [Contact](#contact-)

## About the Repository üìñ

This repository serves as a portfolio of my academic work in statistics, demonstrating my skills in data analysis, statistical modeling, and machine learning. Below are brief descriptions of each project included in this repository. Note that these ressource are non-exhaustive.

## Courses Overview üìÅ

This section details the courses I took during my Master's in Statistics, highlighting the content, techniques used, and tools utilized in each course. The aim is to showcase the breadth and depth of my training and expertise, particularly in Machine Learning, AI, and Statistical Computing.

### Machine Learning and Algorithms

[<img src="/Images/ML/2.png" alt="Page 1 Paper" width="50%">](#)

**Content**: Explored both supervised and unsupervised learning techniques, including classification, regression, clustering, and reinforcement learning. The course emphasized practical application and theoretical understanding of various algorithms.

**Techniques Used**:
- **Supervised Learning**: Including Linear Regression, Logistic Regression, Support Vector Machines (SVMs), Decision Trees, and Ensemble Methods like Random Forests and Gradient Boosting.
- **Unsupervised Learning**: Covering techniques such as K-means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), and Latent Dirichlet Allocation (LDA).
- **Deep Learning**: Encompassing Neural Networks (Feedforward, CNNs, RNNs, LSTM), focusing on applications in image and sequence processing.
- **Reinforcement Learning**: Implementing strategies like Q-learning, SARSA, and Deep Reinforcement Learning with policy gradients and DQN.
- **Optimization Algorithms**: Including Gradient Descent, Stochastic Gradient Descent, and Adam Optimizer, essential for effective model training.

**Tools Used**: Python (Scikit-Learn, TensorFlow, Keras), R (caret, randomForest)

[<img src="/Images/ML/1.png" alt="Page 1 Paper" width="30%">](#)
[<img src="/Images/ML/3.png" alt="Page 1 Paper" width="30%">](#)
[<img src="/Images/ML/4.png" alt="Page 1 Paper" width="30%">](#)

[<img src="/Images/ML/6.png" alt="Page 1 Paper" width="30%">](#)
[<img src="/Images/ML/5.png" alt="Page 1 Paper" width="30%">](#)
[<img src="/Images/ML/7.png" alt="Page 1 Paper" width="30%">](#)

[<img src="/Images/ML/8.png" alt="Page 1 Paper" width="30%">](#)
[<img src="/Images/ML/9.png" alt="Page 1 Paper" width="30%">](#)

### Advanced Theory of Probability & Statistics

**Content**: Advanced topics in probability and inferential statistics, focusing on rigorous mathematical approaches and real-world applications.

**Techniques Used**:
- **Probability Theory**: Foundations, probability distributions, and significant theorems such as the Law of Large Numbers and Central Limit Theorem.
- **Statistical Inference**: Techniques such as Point Estimation (MLE, Method of Moments), Hypothesis Testing (Z-tests, T-tests, Chi-squared tests), and Confidence Intervals.
- **Regression Analysis**: Methods including Simple Linear Regression, Multiple Regression, and Generalized Linear Models (GLMs) with various link functions.
- **Multivariate Statistics**: Analyzing multiple variables using MANOVA and Canonical Correlation Analysis.
- **Non-Parametric Methods**: Including Kernel density estimation, Mann-Whitney U test, Kruskal-Wallis test, and Spearman‚Äôs rank correlation.

**Tools Used**: R, Python

### Statistical Computing

![Image for Statistical Computing](link-to-image)

**Content**: Focus on computational strategies for statistical analysis including simulation, numerical methods, and optimization, essential for handling complex data.

**Techniques Used**: Monte Carlo simulations, Bootstrap methods, EM algorithms, Markov Chain Monte Carlo (MCMC), Optimization algorithms:

- **Monte Carlo Simulations**: Utilized for approximating the probability of complex events and solving quantitative problems. Techniques include:
  - Direct simulation for evaluating integrals and expectations.
  - Importance sampling to reduce variance and improve estimation efficiency.
  - Random walk Monte Carlo for exploring unknown probability distributions.

- **Bootstrap Methods**: Employed for assessing the variability of sample estimates by resampling with replacement from the original data, and for building better predictive models. Includes:
  - Non-parametric bootstrap for resampling data points.
  - Parametric bootstrap involving resampling based on parameter estimates.
  - Block bootstrap for correlated data, commonly used in time series analysis.

- **EM Algorithms** (Expectation-Maximization): A class of iterative algorithms used for finding maximum likelihood estimates in models with latent variables. Applications include:
  - Gaussian Mixture Models for clustering and density estimation.
  - Hidden Markov Models for temporal data analysis.
  - Latent Dirichlet Allocation in natural language processing.

- **Markov Chain Monte Carlo (MCMC)**: A suite of algorithms that sample from probability distributions based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. Key variants include:
  - Metropolis-Hastings algorithm for generating a sequence of samples from a probability distribution from which direct sampling is difficult.
  - Gibbs sampling, particularly useful when combined distributions are known and sampling from the marginal distributions is straightforward.
  - Hamiltonian Monte Carlo, which uses concepts from physics to inform smarter proposals.

- **Optimization Algorithms**: Techniques designed to find the maximum or minimum of an objective function. Employed widely in machine learning, economics, and operations research. Key types include:
  - Gradient Descent and its variants (Stochastic Gradient Descent, Mini-batch Gradient Descent) for minimizing functions by iteratively moving in the direction of steepest descent.
  - Newton's Method and Quasi-Newton Methods (like BFGS and L-BFGS) for more efficiently finding the stationary points of functions using curvature information.
  - Conjugate Gradient Method, used for solving systems of linear equations and nonlinear optimization problems without calculating the Hessian matrix.
  - Simulated Annealing and Genetic Algorithms for problems where the search space is discrete with many local optima.

**Tools Used**: R (Rcpp, shiny), Python (NumPy, SciPy)

### Artificial Intelligence

[<img src="/Images/Other/AI.png" alt="AI" width="30%">](#)

**Content**: Comprehensive study of AI principles and methodologies, including search algorithms, machine learning applications, and neural network architectures.

**Techniques Used**:
- **Classical AI**: Search and optimization techniques such as Depth-First Search (DFS), Breadth-First Search (BFS), A* algorithm, and solving constraint-satisfaction problems.
- **Logic and Knowledge-Based AI**: Utilizing Propositional logic, first-order logic, expert systems, and inference engines with backward and forward chaining.
- **Machine Learning in AI**: Applying learning techniques for classification, prediction, and decision-making in dynamic environments.
- **Natural Language Processing (NLP)**: Techniques like tokenization, parsing, sentiment analysis, Named Entity Recognition (NER), and machine translation.
- **Robotics and Perception**: Algorithms for localization, mapping, navigation, and computer vision tasks like image segmentation and object recognition.

**Tools Used**: Python (TensorFlow, PyTorch, NLTK)

### Bayesian Statistics

[<img src="/Images/Other/BayesianStats.png" alt="Bayesian Stats" width="30%">](#)

**Content**: In-depth exploration of Bayesian statistical methods, covering both theoretical underpinnings and applications in hierarchical modeling and Bayesian networks.

**Techniques Used**:
- **Bayesian Inference**: Fundamental concepts of Bayesian thinking, prior and posterior distributions, and how to update beliefs with new evidence using Bayes' Theorem.
- **Markov Chain Monte Carlo (MCMC)**: Techniques such as the Metropolis-Hastings algorithm and Gibbs sampling to generate samples from complex posterior distributions.
- **Hierarchical Models**: Building multilevel models that reflect the hierarchical structure of data, often used in educational and environmental studies.
- **Bayesian Regression**: Implementation of Bayesian methods in linear and logistic regression models to estimate uncertainty in predictions more accurately.
- **Model Comparison**: Techniques like Bayes Factors and Deviance Information Criterion (DIC) for comparing the fit of different models.
- **Decision Theory**: Using expected loss and utility functions to make decisions based on Bayesian posterior distributions.

**Tools Used**: R (brms, BayesX), Python (PyMC3)

### Statistical Consulting

[<img src="/Images/Other/Page1_Paper.png" alt="Page1 Paper" width="30%">](#)
[<img src="/Images/Other/World_Map.png" alt="World Map" width="50%">](#)

**Content**: Developed in collaboration with Panasonic Aviation Corporation and Black Swan Data, this 5-month long project utilizes advanced Machine Learning models to predict and optimize media selection on flights in order to enahnce passenger satisfaction while minimizing content costs for the airlines.

I dedicated an entired repository for this project, you can explore it here: [https://github.com/LouisBensard/ML-Project01_In-Flight-Media-Optimization.git](https://github.com/LouisBensard/ML-Project01_In-Flight-Media-Optimization.git)

### Multivariate Analysis

[<img src="/Images/Other/Multivariate.png" alt="Multivariate Analysis" width="30%">](#)

**Content**: Techniques for analyzing multiple measurements on each observation, addressing both theory and application.

**Techniques Used**:
- **Principal Component Analysis (PCA)**: Reducing the dimensionality of data by transforming to a new set of variables (principal components), which are linear combinations of the original variables with maximum variance.
- **Factor Analysis**: Identifying latent variables that explain observed correlations among measured variables, often used in psychometrics.
- **Cluster Analysis**: Grouping a set of objects in such a way that objects in the same cluster are more similar to each other than to those in other clusters. Techniques include K-means clustering, hierarchical clustering, and DBSCAN.
- **Discriminant Analysis**: Used for classification and dimensionality reduction, based on modeling differences in groups relative to variables. Includes Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA).
- **Canonical Correlation Analysis (CCA)**: Used to identify and measure the associations between two sets of variables.

**Tools Used**: R, Python (pandas, scikit-learn)

[<img src="/Images/Housing Market Analysis_Lasso_ANOVA_Tukey/1.png" alt="Multivariate Analysis" width="30%">](#)
[<img src="/Images/Housing Market Analysis_Lasso_ANOVA_Tukey/5.png" alt="Multivariate Analysis" width="30%">](#)
[<img src="/Images/Housing Market Analysis_Lasso_ANOVA_Tukey/3.png" alt="Multivariate Analysis" width="30%">](#)

[<img src="/Images/Housing Market Analysis_Lasso_ANOVA_Tukey/4.png" alt="Multivariate Analysis" width="30%">](#)
[<img src="/Images/Housing Market Analysis_Lasso_ANOVA_Tukey/2.png" alt="Multivariate Analysis" width="30%">](#)
[<img src="/Images/Housing Market Analysis_Lasso_ANOVA_Tukey/6.png" alt="Multivariate Analysis" width="30%">](#)

[<img src="/Images/Housing Market Analysis_Lasso_ANOVA_Tukey/7.png" alt="Multivariate Analysis" width="30%">](#)

### Categorical Data Analysis

**Content**: Statistical methods for categorical data, including logistic regression, and models for count data.

**Techniques Used**:
- **Logistic Regression**: Modeling binary and multinomial outcomes to predict probabilities of different categories, with applications in risk modeling and other areas where outcomes are discrete.
- **Probit and Logit Models**: Similar to logistic regression but using the probit and logit link functions, respectively, to model the probability of a binary response based on predictor variables.
- **Poisson and Negative Binomial Regression**: Modeling count data where outputs are counts or rates, used extensively in public health and insurance statistics.
- **Contingency Tables and Chi-Square Tests**: Analyzing the relationship between categorical variables using cross-tabulations and testing independence with chi-square tests.
- **Generalized Linear Models (GLM)**: Extending linear models to allow for response variables that have error distribution models other than a normal distribution, commonly used for categorical and count data.

**Tools Used**: R, Python (statsmodels)

### Experimental Design

**Content**: Study of designing experiments to address specific research questions, including the analysis of variance and covariance structures.

**Techniques Used**:
- **Analysis of Variance (ANOVA)**: Testing differences in means across multiple groups, including one-way and two-way ANOVA for understanding interaction effects between factors.
- **Factorial Designs**: Structuring experiments where multiple factors are investigated simultaneously. This includes full factorial and fractional factorial designs where some factor combinations are omitted to reduce complexity.
- **Block and Randomized Designs**: Minimizing the effects of nuisance variables by blocking and random assignment of treatments to subjects.
- **Covariance Analysis (ANCOVA)**: Extending ANOVA to include covariates that might influence the dependent variable, allowing for adjustment of the dependent variable based on these covariates.
- **Response Surface Methodology (RSM)**: Used for exploring optimum conditions in experiments involving several variables and responses. It helps in fitting a quadratic surface and determining the optimal conditions.

**Tools Used**: R

### Applied BioStatistics

**Content**: Application of statistical techniques to biological data, particularly in health sciences, covering survival analysis, and repeated measures.

**Techniques Used**:
- **Survival Analysis**: Modeling time-to-event data, allowing for the analysis of survival rates and factors affecting survival such as pharmaceuticals or environmental risks.
- **Cox Proportional Hazards Model**: A semiparametric model used to estimate the hazard ratio in survival analysis, considering the effect of several variables on survival.
- **Repeated Measures ANOVA**: Analyzing data collected from the same subjects under different conditions or over several time points, often used in clinical trials to assess treatment effects over time.
- **Logistic Regression for Dichotomous Outcomes**: Applied specifically in medical statistics for outcomes that have two states, such as presence or absence of disease.
- **Meta-Analysis**: Combining results from multiple studies to improve power and estimate effect size more accurately, essential in evidence-based healthcare.

**Tools Used**: R

## Technologies Used üíª

### Python Libraries
- **Pandas**: Essential for data manipulation and analysis, providing data structures and operations for manipulating numerical tables and time series.
- **NumPy**: Fundamental package for scientific computing with Python, supporting large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions.
- **Scikit-Learn**: Simple and efficient tools for predictive data analysis, built on NumPy, SciPy, and matplotlib, supporting various classification, regression, and clustering algorithms.
- **TensorFlow**: An end-to-end open-source platform for machine learning, facilitating the creation of deep learning models.
- **Keras**: High-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano, designed to enable fast experimentation with deep neural networks.
- **Matplotlib**: A plotting library for creating static, interactive, and animated visualizations in Python.
- **Seaborn**: Based on matplotlib, this library provides a high-level interface for drawing attractive and informative statistical graphics.
- **Statsmodels**: Module that allows users to explore data, estimate statistical models, and perform statistical tests.
- **PyMC3**: Bayesian statistical modeling focusing on advanced Markov chain Monte Carlo and variational fitting algorithms.
- **NLTK**: Leading platform for building Python programs to work with human language data, supporting tasks such as classification, tokenization, stemming, tagging, parsing, and semantic reasoning.

### R Packages
- **ggplot2**: Part of the tidyverse, widely used for creating complex, multi-plot graphics that are kitted out with all necessary details like legends and labels.
- **dplyr**: A grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges.
- **caret**: Short for Classification And REgression Training, this package provides a suite of tools that streamline the process of training and tuning machine learning models.
- **randomForest**: Implements the random forest algorithm for classification and regression, an ensemble method based on aggregated decision trees.
- **lme4**: Provides functions to fit and analyze mixed linear and nonlinear effects models, often used in complex experimental designs.
- **shiny**: Makes it incredibly easy to build interactive web applications with R for visualizing data and models dynamically.
- **BayesX**: Software package that performs Bayesian inference in structured additive regression models and is integrated within R.
- **brms**: An R package for Bayesian multilevel models using Stan, which allows fitting complex models using custom families and link functions.
- **factoextra**: Used for extracting and visualizing the results of multivariate data analyses, including PCA, CA, MCA, and clustering.
- **survival**: A package that contains the core survival analysis routines, including defining and fitting Cox models, Kaplan-Meier plots, and other routines.
